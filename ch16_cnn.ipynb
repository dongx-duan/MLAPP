{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses CIFAR-10 data set. It can be downloaded at:\n",
    "http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "Alex Krizhevsky wrote a tech report\n",
    "http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "base_path = '/home/hadoop/data/cifar-10/cifar-10-batches-py'\n",
    "train_files = ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5']\n",
    "test_files = ['test_batch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "      \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "      num_labels = labels_dense.shape[0]\n",
    "      index_offset = numpy.arange(num_labels) * num_classes\n",
    "      labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "      labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "      return labels_one_hot\n",
    "\n",
    "def load_data(files):\n",
    "    # batches{'data', 'labels', 'batch_label', 'filenames'}\n",
    "    batches = [unpickle(os.path.join(base_path, f)) for f in files]\n",
    "    data = numpy.vstack((d['data'] for d in batches))\n",
    "    data = numpy.reshape(data, [-1, 3, 32, 32]) / 255.0\n",
    "    data = numpy.transpose(data, [0,2,3,1])\n",
    "    label = numpy.vstack([numpy.expand_dims(numpy.asarray(d['labels']), axis=1) for d in batches])\n",
    "    return data, dense_to_one_hot(label, 10)\n",
    "\n",
    "train_data, train_label = load_data(train_files)\n",
    "assert(train_data.shape == (50000, 32, 32, 3))\n",
    "assert(train_label.shape == (50000, 10))\n",
    "\n",
    "test_data, test_label = load_data(test_files)\n",
    "assert(test_data.shape == (10000, 32, 32, 3))\n",
    "assert(test_label.shape == (10000, 10))\n",
    "\n",
    "def generate_batch(batch_size):\n",
    "    perm = numpy.random.permutation(numpy.arange(len(train_data)))\n",
    "    _train = train_data[perm]\n",
    "    _label = train_label[perm]\n",
    "    k = 1\n",
    "    while k*batch_size <= len(train_data):\n",
    "        start = (k-1) * batch_size\n",
    "        end = k * batch_size\n",
    "        yield _train[start:end], _label[start:end]\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(is_training = True):\n",
    "    # helper function for full connect layer\n",
    "    def fc(name_scope, input_size, output_size, x):\n",
    "        with tf.variable_scope(name_scope):\n",
    "            w = tf.get_variable('w', [input_size, output_size], \n",
    "                                initializer=tf.random_uniform_initializer(minval=-1.0, maxval=1.0))\n",
    "            b = tf.get_variable('b', [output_size], \n",
    "                                initializer=tf.constant_initializer(1))\n",
    "            return tf.matmul(x,w) + b\n",
    "    \n",
    "    def conv(name_scope, filter_size, x):\n",
    "        with tf.variable_scope(name_scope):\n",
    "            w = tf.get_variable('filter', filter_size, \n",
    "                                initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            c = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "            \n",
    "            b = tf.get_variable('b', [filter_size[-1]], \n",
    "                               initializer=tf.constant_initializer(1.0))\n",
    "            return tf.nn.relu(tf.nn.bias_add(c, b))\n",
    "            \n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    conv1 = conv('conv1', [5, 5, 3, 32], x)\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    norm1 = tf.nn.local_response_normalization(pool1, 4, bias=1.0, \n",
    "                                               alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "    \n",
    "    conv2 = conv('conv2', [5, 5, 32, 64], norm1)\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    norm2 = tf.nn.local_response_normalization(pool2, 4, bias=1.0, \n",
    "                                               alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    \n",
    "#     pool_shape = pool2.get_shape().as_list()\n",
    "#     shape = tf.Variable( [pool_shape[0], pool_shape[1]*pool_shape[2]*pool_shape[3]] )\n",
    "\n",
    "    reshaped_pool = tf.reshape(norm2, shape=[-1, 5 * 5 * 64])\n",
    "    \n",
    "    fc1 = tf.nn.sigmoid(fc('fc1', 5 * 5 * 64, 256, reshaped_pool))\n",
    "    if is_training:\n",
    "        fc1 = tf.nn.dropout(fc1, keep_prob=0.5)\n",
    "    \n",
    "    fc2 = fc('fc2', 256, 10, fc1)\n",
    "    logits = tf.nn.softmax(fc2)\n",
    "    \n",
    "    loss = -tf.reduce_mean(y * tf.log(logits))\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(logits,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # cifar-10 has only 10 classes\n",
    "    # top_5 error below is only a reference; it's useful for ImageNet dataset\n",
    "    top_5 = tf.nn.in_top_k(logits, tf.argmax(y,1), 5)\n",
    "    accuracy_top_5 = tf.reduce_mean(tf.cast(top_5, tf.float32))\n",
    "    \n",
    "    if not is_training:\n",
    "        return x, y, loss, accuracy, accuracy_top_5\n",
    "    \n",
    "    lr = 0.01\n",
    "    train_step = tf.train.MomentumOptimizer(lr, 0.9).minimize(loss)\n",
    "    return x, y, logits, loss, train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 epoch --> train_loss: 0.178052  test_loss: 0.163174  top1: 0.416  top5: 0.903\n",
      "200 epoch --> train_loss: 0.161765  test_loss: 0.145139  top1: 0.476  top5: 0.927\n",
      "300 epoch --> train_loss: 0.149602  test_loss: 0.139257  top1: 0.501  top5: 0.929\n",
      "400 epoch --> train_loss: 0.141485  test_loss: 0.127108  top1: 0.551  top5: 0.947\n",
      "500 epoch --> train_loss: 0.136091  test_loss: 0.124225  top1: 0.564  top5: 0.949\n",
      "600 epoch --> train_loss: 0.130656  test_loss: 0.118713  top1: 0.584  top5: 0.953\n",
      "700 epoch --> train_loss: 0.127984  test_loss: 0.121745  top1: 0.576  top5: 0.943\n",
      "800 epoch --> train_loss: 0.124251  test_loss: 0.114829  top1: 0.589  top5: 0.959\n",
      "900 epoch --> train_loss: 0.121454  test_loss: 0.112562  top1: 0.600  top5: 0.960\n",
      "1000 epoch --> train_loss: 0.119200  test_loss: 0.110812  top1: 0.606  top5: 0.961\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default(), tf.Session() as session:\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        x_train, y_train, train_logits, train_loss, train_step = cnn_model()\n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        x_test, y_test, test_loss, top1, top5 = cnn_model(is_training=False)\n",
    "    \n",
    "    session.run(tf.initialize_all_variables())\n",
    "    batch_size = 512\n",
    "    \n",
    "    for i in range(1, 1001):\n",
    "        _train_loss = 0\n",
    "        batch_count = 0\n",
    "        for _d, _l in generate_batch(batch_size):\n",
    "            _loss, _ = session.run([train_loss, train_step], feed_dict={x_train:_d, y_train:_l})\n",
    "            _train_loss += _loss\n",
    "            batch_count += 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            _test_loss, _top1, _top5 = session.run([test_loss, top1, top5], \n",
    "                                    feed_dict={x_test:test_data, y_test:test_label})\n",
    "\n",
    "            print(\"{} epoch --> train_loss: {:.6f}  test_loss: {:.6f}  top1: {:.3f}  top5: {:.3f}\".format(\n",
    "                i, _train_loss/batch_count, _test_loss, _top1, _top5) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
